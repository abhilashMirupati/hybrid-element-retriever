# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jDNvUO2BWdz3HPcfrwRNV-9UaKAtdzrq
"""

# Colab-ready single-file pipeline: Playwright DOM extraction -> Canonical builder -> Gemma embeddings -> similarity
# -> OCR / Pix2Struct fallback -> returns best element + diagnostics
# Paste into a single Colab cell and run. (Replace any API keys if you choose to use a cloud Pix2Struct later.)

# ---------------------------
# 0) Install required packages
# ---------------------------
# Note: running these installs in Colab may take a few minutes.
!pip install --upgrade pip -q
!apt-get update -qq
!apt-get install -y -qq tesseract-ocr libtesseract-dev
!pip install -q playwright sentence-transformers transformers pillow pytesseract lxml bs4 hf
!playwright install --with-deps chromium

# ---------------------------
# 1) Imports
# ---------------------------
import asyncio
import base64
import io
import asyncio, time, json
import json
import hashlib
from typing import List, Dict, Any, Tuple
import hf
from PIL import Image
import numpy as np

from sentence_transformers import SentenceTransformer, util
from playwright.async_api import async_playwright
import pytesseract
from bs4 import BeautifulSoup

import nest_asyncio
nest_asyncio.apply()

!hf auth login

# =============================
# 2. Canonical Builder Helpers
# =============================

def build_canonical_json(node, frame_id, frame_url, ancestor_levels=2, max_siblings=1):
    """Build structured JSON for a node with ancestors/siblings/frame info."""
    def safe_attr(attrs, keys=None):
        if not attrs: return {}
        if keys:
            return {k: attrs.get(k, "") for k in keys if k in attrs}
        return attrs

    canonical = {
        "frameId": frame_id,
        "frameUrl": frame_url,
        "node": {
            "tag": node["tag"],
            "text": node.get("text", "").strip(),
            "attrs": safe_attr(node.get("attrs", {})),
            "bbox": node.get("bbox", [])
        },
        "ancestors": [],
        "siblings": []
    }

    # Add ancestors (up to N)
    ancestors = node.get("ancestors", [])[:ancestor_levels]
    for anc in ancestors:
        canonical["ancestors"].append({
            "tag": anc["tag"],
            "attrs": safe_attr(anc.get("attrs", {}))
        })

    # Add siblings (up to N left & right)
    siblings = node.get("siblings", [])[:max_siblings]
    for sib in siblings:
        canonical["siblings"].append({
            "tag": sib["tag"],
            "text": sib.get("text", "").strip()
        })

    return canonical


def flatten_canonical_for_embedding(canonical_json):
    """Convert canonical JSON into a compact string for embeddings."""
    parts = [f"FRAME[{canonical_json['frameId']}]"]

    for anc in canonical_json["ancestors"]:
        parts.append(f"ANC[{anc['tag']} {json.dumps(anc['attrs'])}]")

    node = canonical_json["node"]
    node_attrs = " ".join([f"{k}={v}" for k, v in node["attrs"].items()])
    parts.append(f"NODE[{node['tag']} text={node['text']} {node_attrs}]")

    for sib in canonical_json["siblings"]:
        parts.append(f"SIB[{sib['tag']} text={sib['text']}]")

    return " | ".join(parts)

# REPLACE your existing extract_elements with this improved version

async def _extract_elements_per_frame(page, frame, target_text=None, ancestor_levels=2, max_siblings=1, include_hidden=False):
    """
    Run inside a frame: walk the DOM (including shadow roots) and return matches.
    Returns list of dicts: {
      tag, text, attrs, bbox, frame_id, frame_url, outerHTML
    }
    """
    js = r"""
    (args) => {
      const targetText = args.targetText;
      const includeHidden = !!args.includeHidden;

      function isTrulyHidden(el){
        if (!el) return true;
        try {
          const s = window.getComputedStyle(el);
          if (!includeHidden && (s.display === 'none' || s.visibility === 'hidden' || s.opacity === '0')) return true;
          if (el.hasAttribute && el.hasAttribute('aria-hidden') && el.getAttribute('aria-hidden') === 'true') return true;
        } catch (e) { return true; }
        return false;
      }

      // recursively walk DOM and shadow roots
      function* deepWalker(root){
        const stack = [root];
        while(stack.length){
          const node = stack.shift();
          if (!node) continue;
          if (node.nodeType === Node.ELEMENT_NODE){
            yield node;
            // push children (normal)
            for (const c of Array.from(node.children || [])) stack.push(c);
            // push shadow root children if present
            if (node.shadowRoot){
              for (const sc of Array.from(node.shadowRoot.children || [])) stack.push(sc);
            }
          }
        }
      }

      const out = [];
      const body = document.body;
      if (!body) return out;

      for (const el of deepWalker(body)){
        try {
          const txt = (el.innerText || '').trim();
          if (targetText !== null && targetText !== undefined && targetText !== '') {
            if (txt !== targetText) continue;
          }
          if (isTrulyHidden(el)) continue;

          const attrs = {};
          for (let i=0;i<el.attributes.length;i++){
            const a = el.attributes[i];
            attrs[a.name] = a.value;
          }
          let rect = {left:0, top:0, width:0, height:0};
          try {
            const r = el.getBoundingClientRect();
            rect = { left: r.left, top: r.top, width: r.width, height: r.height };
          } catch (e) {}

          out.push({
            tag: el.tagName,
            text: txt,
            attrs: attrs,
            css: {
              display: window.getComputedStyle(el).display,
              visibility: window.getComputedStyle(el).visibility,
              opacity: window.getComputedStyle(el).opacity
            },
            bbox: rect,
            outerHTML: el.outerHTML ? el.outerHTML : ''
          });
        } catch(e){
          // ignore nodes we can't access
        }
      }
      return out;
    }
    """
    # run the JS in frame context
    try:
        matches = await frame.evaluate(js, {
            "targetText": target_text,
            "includeHidden": include_hidden
        })
    except Exception as e:
        # If evaluation fails, return empty
        print("Frame evaluate error:", e)
        matches = []

    # annotate with frame metadata
    annotated = []
    for m in matches:
        annotated.append({
            "tag": m.get("tag"),
            "text": m.get("text"),
            "attrs": m.get("attrs"),
            "css": m.get("css"),
            "bbox": m.get("bbox"),
            "outerHTML": m.get("outerHTML"),
            "frameId": str(id(frame)),
            "frameUrl": frame.url
        })
    return annotated


async def extract_elements(url, ancestor_levels=2, max_siblings=1, include_hidden=False, timeout=60000):
    """
    Replacement top-level extractor.
    Returns: (results_list, timings_dict)
    """
    results = []
    timings = {}
    t0 = time.time()
    async with async_playwright() as pw:
        browser = await pw.chromium.launch(headless=True)
        page = await browser.new_page()
        # wait until network idle to let JS load content
        await page.goto(url, wait_until="networkidle", timeout=timeout)
        timings["page_load"] = time.time() - t0

        # gather frames
        t1 = time.time()
        frames = page.frames
        timings["num_frames"] = len(frames)
        timings["frames_list"] = [f.url for f in frames]
        timings["frame_collect_time"] = time.time() - t1

        # iterate frames and run extraction
        t2 = time.time()
        for frame in frames:
            try:
                matches = await _extract_elements_per_frame(page, frame, target_text=None, ancestor_levels=ancestor_levels, max_siblings=max_siblings, include_hidden=include_hidden)
                # If user wants to filter by a specific target text later, we filtered by targetText==None so we return all elements
                # We'll filter by target_text in Python or later
                for m in matches:
                    results.append(m)
            except Exception as e:
                # log and continue
                print("Error extracting frame:", frame.url, e)
        timings["per_frame_extract_time"] = time.time() - t2

        # optionally deduplicate by (tag,text,attrs,bbox) to reduce duplicates
        t3 = time.time()
        seen = set()
        deduped = []
        for r in results:
            key = (r.get("tag",""), r.get("text",""), json.dumps(r.get("attrs",{}), sort_keys=True), int(r.get("bbox",{}).get("left",0)), int(r.get("bbox",{}).get("top",0)))
            if key in seen:
                continue
            seen.add(key)
            deduped.append(r)
        timings["dedupe_time"] = time.time() - t3

        # close browser
        await browser.close()

    timings["total_extraction_time"] = time.time() - t0
    return deduped, timings

# =============================
# 4. Embedding & Retrieval
# =============================

def retrieve_best_element(url, query, target_text="Apple",
                          ancestor_levels=2, max_siblings=1):
    # Extract DOM
    results, timings = asyncio.run(
        extract_elements(url=url, ancestor_levels=ancestor_levels, max_siblings=max_siblings)
    )

    # Debugging: Print the number of results and the results themselves
    print(f"Number of extracted elements: {len(results)}")
    # print("Extracted elements:", json.dumps(results, indent=2)) # Uncomment for detailed inspection

    # Build embeddings
    candidates = [r["canonical_str"] for r in results]

    # Check if there are any candidates before proceeding
    if not candidates:
        return {
            "best_index": -1,
            "best_canonical": None,
            "best_score": 0.0,
            "diagnostics": {
                "timings": timings,
                "num_candidates": len(candidates),
                "message": "No visible elements extracted from the page."
            }
        }

    # Check if model is defined, if not initialize it
    if 'model' not in globals():
        global model
        model = SentenceTransformer('all-MiniLM-L6-v2')

    query_embedding = model.encode([query])[0]
    cand_embeddings = model.encode(candidates)

    import numpy as np
    sims = np.dot(cand_embeddings, query_embedding) / (
        np.linalg.norm(cand_embeddings, axis=1) * np.linalg.norm(query_embedding)
    )

    best_idx = int(np.argmax(sims))
    return {
        "best_index": best_idx,
        "best_canonical": results[best_idx]["canonical_json"],
        "best_score": float(sims[best_idx]),
        "diagnostics": {
            "timings": timings,
            "num_candidates": len(candidates)
        }
    }

# =============================
# 5. Run Example
# =============================

if __name__ == "__main__":
    url = "https://www.verizon.com/smartphones/"
    query = "Click on Apple button in footer"

    result = retrieve_best_element(url, query,
                                   target_text="Apple",
                                   ancestor_levels=2,
                                   max_siblings=1)

    print(json.dumps(result, indent=2))