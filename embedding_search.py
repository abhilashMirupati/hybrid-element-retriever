# -*- coding: utf-8 -*-
"""Embedding search

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17Mxuv-wKL3zgFwyAO_BH2-1NP4JO0Tor
"""

# Colab-ready single-file pipeline: Playwright DOM extraction -> Canonical builder -> Gemma embeddings -> similarity
# -> OCR / Pix2Struct fallback -> returns best element + diagnostics
# Paste into a single Colab cell and run. (Replace any API keys if you choose to use a cloud Pix2Struct later.)

# ---------------------------
# 0) Install required packages
# ---------------------------
# Note: running these installs in Colab may take a few minutes.
!pip install --upgrade pip -q
!apt-get update -qq
!apt-get install -y -qq tesseract-ocr libtesseract-dev
!pip install -q playwright sentence-transformers transformers pillow pytesseract lxml bs4 hf
!playwright install --with-deps chromium

# ---------------------------
# 1) Imports
# ---------------------------
import asyncio
import base64
import io
import json
import hashlib
from typing import List, Dict, Any, Tuple
import hf
from PIL import Image
import numpy as np

from sentence_transformers import SentenceTransformer, util
from playwright.async_api import async_playwright
import pytesseract
from bs4 import BeautifulSoup

!hf auth login

# ---------------------------
# 2) Config / Model load
# ---------------------------
# Embedding model (Gemma)
EMBED_MODEL_NAME = "google/embeddinggemma-300m"
print("Loading embedding model:", EMBED_MODEL_NAME)
embed_model = SentenceTransformer(EMBED_MODEL_NAME)

# Similarity thresholds & heuristics
SIMILARITY_THRESHOLD = 0.72   # if top score < this -> fallback
AMBIGUITY_DELTA = 0.05       # if top - second < delta -> fallback
MAX_CANONICAL_ATTRS_LEN = 400  # truncate long attribute dumps
ANCESTOR_LEVELS_DEFAULT = 2
MAX_SIBLINGS_DEFAULT = 1

# ---------------------------
# 3) JS extractor (Playwright) - returns canonical nodes + screenshot (base64)
# ---------------------------
import nest_asyncio
nest_asyncio.apply()

JS_EXTRACTOR = r"""
(args) => {
  const targetText = args.targetText;
  const ancestorLevels = Number(args.ancestorLevels) || 2;
  const maxSiblings = Number(args.maxSiblings) || 0;
  const includeHidden = !!args.includeHidden;
  const results = [];

  function isTrulyHidden(el) {
    if (!el) return true;
    try {
      const style = window.getComputedStyle(el);
      const rect = el.getBoundingClientRect();
      if (!includeHidden && (style.display === 'none' || style.visibility === 'hidden' || style.opacity === '0')) return true;
      if (!includeHidden && (rect.width === 0 && rect.height === 0)) return true;
    } catch(e) {
      return true;
    }
    return false;
  }

  function serializeNode(el) {
    if (!el) return null;
    const attrs = {};
    for (let i = 0; i < el.attributes.length; i++) {
      attrs[el.attributes[i].name] = el.attributes[i].value;
    }
    const style = window.getComputedStyle(el);
    const rect = el.getBoundingClientRect();
    const css = {
      display: style.display,
      visibility: style.visibility,
      opacity: style.opacity,
      position: style.position,
      zIndex: style.zIndex,
      left: rect.left,
      top: rect.top,
      width: rect.width,
      height: rect.height
    };
    return {
      tag: el.tagName,
      innerText: (el.innerText || '').trim(),
      attrs: attrs,
      css: css,
      outerHTML: el.outerHTML
    };
  }

  function buildCanonical(el) {
    const node = serializeNode(el);
    const ancestors = [];
    let cur = el.parentElement;
    let cnt = 0;
    while (cur && cur !== document.documentElement && cnt < ancestorLevels) {
      ancestors.unshift(serializeNode(cur));
      cur = cur.parentElement;
      cnt++;
    }
    const siblings = [];
    if (el.parentElement && maxSiblings > 0) {
      const kids = Array.from(el.parentElement.children);
      const idx = kids.indexOf(el);
      const start = Math.max(0, idx - maxSiblings);
      const end = Math.min(kids.length - 1, idx + maxSiblings);
      for (let i = start; i <= end; i++) {
        if (i === idx) continue;
        siblings.push(serializeNode(kids[i]));
      }
    }
    return { node, ancestors, siblings };
  }

  const walker = document.createTreeWalker(document.body, NodeFilter.SHOW_ELEMENT, null, false);
  let n;
  while (n = walker.nextNode()) {
    try {
      if (((n.innerText || '').trim() === targetText) && !isTrulyHidden(n)) {
        results.push(buildCanonical(n));
      }
    } catch(e) { /* ignore node errors */ }
  }
  return results;
}
"""

async def _extract_canonical_nodes_async(url: str, target_text: str,
                                         ancestor_levels: int = ANCESTOR_LEVELS_DEFAULT,
                                         max_siblings: int = MAX_SIBLINGS_DEFAULT,
                                         include_hidden: bool = False,
                                         timeout: int = 60000) -> Tuple[List[Dict[str, Any]], str]:
    """
    Returns:
      - list of canonical nodes (dicts with 'node','ancestors','siblings')
      - screenshot base64 (full page)
    """
    async with async_playwright() as pw:
        browser = await pw.chromium.launch(headless=True)
        page = await browser.new_page()
        await page.goto(url, wait_until="networkidle", timeout=timeout)
        # execute extraction
        canonical_nodes = await page.evaluate(JS_EXTRACTOR, {
            "targetText": target_text,
            "ancestorLevels": ancestor_levels,
            "maxSiblings": max_siblings,
            "includeHidden": include_hidden
        })
        # screenshot (full page)
        screenshot_bytes = await page.screenshot(full_page=True)
        screenshot_b64 = base64.b64encode(screenshot_bytes).decode("ascii")
        await browser.close()
    return canonical_nodes, screenshot_b64

def extract_canonical_nodes(url: str, target_text: str, ancestor_levels=ANCESTOR_LEVELS_DEFAULT, max_siblings=MAX_SIBLINGS_DEFAULT, include_hidden=False):
    return asyncio.get_event_loop().run_until_complete(
        _extract_canonical_nodes_async(url, target_text, ancestor_levels, max_siblings, include_hidden)
    )

# ---------------------------
# 4) Helpers: safe attrs string, flatten canonical (truncation + bbox)
# ---------------------------
import json, math

def safe_attrs_str(attrs: Dict[str, Any], max_len: int = MAX_CANONICAL_ATTRS_LEN) -> str:
    s = json.dumps(attrs, sort_keys=True, ensure_ascii=False)
    if len(s) <= max_len:
        return s
    h = hashlib.sha1(s.encode('utf-8')).hexdigest()[:8]
    return s[:max_len] + "...|sha1:" + h

def flatten_canonical_item(canonical: Dict[str, Any]) -> str:
    parts = []
    # ancestors (closest first)
    for anc in canonical.get("ancestors", []):
        css = anc.get("css", {})
        parts.append(
            f"ANC[{anc.get('tag')}] attrs={safe_attrs_str(anc.get('attrs', {}))} bbox={int(css.get('left',0))},{int(css.get('top',0))},{int(css.get('width',0))}x{int(css.get('height',0))}"
        )
    n = canonical.get("node", {})
    ncss = n.get("css", {})
    parts.append(
        f"NODE[{n.get('tag')}] text={ (n.get('innerText') or '')[:200] } attrs={safe_attrs_str(n.get('attrs', {}))} bbox={int(ncss.get('left',0))},{int(ncss.get('top',0))},{int(ncss.get('width',0))},{int(ncss.get('height',0))}"
    )
    for s in canonical.get("siblings", []):
        parts.append(
            f"SIB[{s.get('tag')}] text={ (s.get('innerText') or '')[:80] } attrs={safe_attrs_str(s.get('attrs', {}), max_len=200)} bbox={int(s.get('css', {}).get('left',0))},{int(s.get('css', {}).get('top',0))},{int(s.get('css', {}).get('width',0))},{int(s.get('css', {}).get('height',0))}"
        )
    flat = " | ".join(parts)
    # final safety: if extremely long, truncate but keep hash
    if len(flat) > 32000:
        h = hashlib.sha1(flat.encode('utf-8')).hexdigest()[:8]
        return flat[:32000] + "...|sha1:" + h
    return flat

# ---------------------------
# 5) Rank with Gemma embeddings
# ---------------------------
def rank_candidates_with_gemma(canonical_nodes: List[Dict[str, Any]], query: str):
    if not canonical_nodes:
        return [], np.array([])
    canonical_strings = [flatten_canonical_item(c) for c in canonical_nodes]
    # encode
    cand_emb = embed_model.encode(canonical_strings, convert_to_tensor=True)
    q_emb = embed_model.encode([query], convert_to_tensor=True)
    sims = util.cos_sim(q_emb, cand_emb)[0].cpu().numpy()
    return canonical_strings, sims

# ---------------------------
# 6) OCR-based visual fallback (serves as Pix2Struct fallback if HF not available)
# ---------------------------
def ocr_find_text_bbox(screenshot_b64: str, query: str) -> List[Tuple[int,int,int,int,str]]:
    """
    Returns list of OCR-detected bounding boxes (left, top, width, height, text) whose text contains query (case-insensitive).
    Uses pytesseract image_to_data.
    """
    img_bytes = base64.b64decode(screenshot_b64)
    img = Image.open(io.BytesIO(img_bytes)).convert("RGB")
    data = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT)
    matches = []
    for i, text in enumerate(data['text']):
        if not text: continue
        if query.lower() in text.lower():
            left = int(data['left'][i])
            top = int(data['top'][i])
            width = int(data['width'][i])
            height = int(data['height'][i])
            matches.append((left, top, width, height, text))
    return matches

def iou_bbox(b1, b2):
    # b = (left, top, w, h)
    l1,t1,w1,h1 = b1; l2,t2,w2,h2 = b2
    r1, b1y = l1+w1, t1+h1
    r2, b2y = l2+w2, t2+h2
    ix = max(0, min(r1,r2) - max(l1,l2))
    iy = max(0, min(b1y,b2y) - max(t1,t2))
    inter = ix*iy
    area1 = w1*h1
    area2 = w2*h2
    union = area1+area2-inter if (area1+area2-inter)>0 else 1
    return inter/union

def map_ocr_to_candidates(ocr_bboxes, canonical_nodes):
    """
    For each OCR bbox, find candidate node with highest IoU. Return mapping (ocr_idx -> best_candidate_idx, iou)
    """
    mapping = []
    for ox, (l,t,w,h,txt) in enumerate(ocr_bboxes):
        best_iou = 0.0
        best_idx = None
        for ci, c in enumerate(canonical_nodes):
            n = c.get("node", {})
            css = n.get("css", {})
            cb = (int(css.get("left",0)), int(css.get("top",0)), int(css.get("width",0)), int(css.get("height",0)))
            iou = iou_bbox((l,t,w,h), cb)
            if iou > best_iou:
                best_iou = iou
                best_idx = ci
        mapping.append((ox, best_idx, best_iou))
    return mapping

# ---------------------------
# 7) High-level runner orchestrating extraction -> ranking -> fallback
# ---------------------------
def retrieve_best_element(url: str,
                          target_text: str,
                          query: str = None,
                          ancestor_levels: int = ANCESTOR_LEVELS_DEFAULT,
                          max_siblings: int = MAX_SIBLINGS_DEFAULT,
                          include_hidden: bool = False,
                          similarity_threshold: float = SIMILARITY_THRESHOLD,
                          ambiguity_delta: float = AMBIGUITY_DELTA):
    """
    Returns dict with:
      - best_candidate_index (or None)
      - best_candidate_canonical (string)
      - best_candidate_node (full canonical dict)
      - similarity_scores (list)
      - fallback_used (bool), fallback_details
      - diagnostics (self-critique info)
    """
    q = query or target_text
    canonical_nodes, screenshot_b64 = extract_canonical_nodes(url, target_text, ancestor_levels, max_siblings, include_hidden)
    diagnostics = {
        "num_extracted_candidates": len(canonical_nodes),
        "ancestor_levels": ancestor_levels,
        "max_siblings": max_siblings,
        "include_hidden": include_hidden
    }

    if not canonical_nodes:
        # no exact text matches in DOM -> go to visual fallback (OCR/Pix2Struct)
        ocr_matches = ocr_find_text_bbox(screenshot_b64, q)
        diagnostics["ocr_matches"] = len(ocr_matches)
        fallback = {"type": "ocr_only", "ocr_matches": ocr_matches}
        return {"best_index": None, "best_canonical": None, "best_node": None, "scores": [], "fallback_used": True, "fallback": fallback, "diagnostics": diagnostics}

    canonical_strings, sims = rank_candidates_with_gemma(canonical_nodes, q)
    # compute top 2
    idx_sorted = np.argsort(-sims)
    best_idx = int(idx_sorted[0])
    best_score = float(sims[best_idx])
    second_score = float(sims[idx_sorted[1]]) if len(sims) > 1 else 0.0

    diagnostics.update({
        "best_score": best_score,
        "second_score": second_score,
        "num_candidates": len(sims)
    })

    # Decide fallback conditions
    fallback_needed = False
    fallback_reason = None
    if best_score < similarity_threshold:
        fallback_needed = True
        fallback_reason = "low_top_score"
    elif (best_score - second_score) < ambiguity_delta:
        fallback_needed = True
        fallback_reason = "ambiguous_top_two"

    if not fallback_needed:
        # success
        return {
            "best_index": best_idx,
            "best_canonical": canonical_strings[best_idx],
            "best_node": canonical_nodes[best_idx],
            "scores": sims.tolist(),
            "fallback_used": False,
            "fallback": None,
            "diagnostics": diagnostics
        }

    # Fallback flow: try Pix2Struct if available, else OCR
    # Attempt to call HF pix2struct (best-effort). If fails, do OCR mapping.
    fallback_info = {"reason": fallback_reason}
    try:
        # Attempt to use a HF Pix2Struct-ish model if available (best-effort).
        # Many repo-specific processors are required; if not available, this block will raise and we fallback to OCR below.
        from transformers import AutoProcessor, VisionEncoderDecoderModel
        # NOTE: model id may vary; this is an attempt placeholder. If not available, except branch will run.
        PIX_MODEL = "google/pix2struct-base"  # attempt only; many pix2struct variants exist
        proc = AutoProcessor.from_pretrained(PIX_MODEL)
        v2t = VisionEncoderDecoderModel.from_pretrained(PIX_MODEL)
        # prepare image
        img_bytes = base64.b64decode(screenshot_b64)
        image = Image.open(io.BytesIO(img_bytes)).convert("RGB")
        pixel_values = proc(images=image, return_tensors="pt").pixel_values
        # create prompt wrapper containing the query (model-specific)
        gen_kwargs = {"max_length": 256, "num_beams": 4}
        outputs = v2t.generate(pixel_values, **gen_kwargs)
        text_out = proc.batch_decode(outputs, skip_special_tokens=True)[0]
        fallback_info["pix2struct_raw"] = text_out
        # Attempt to parse bbox-like output: many pix2struct models return textual answers; mapping is model-specific.
        # We'll include raw text; user can map manually. Then fallback to OCR mapping as well for bbox selection.
    except Exception as e:
        fallback_info["pix2struct_error"] = str(e)

    # OCR mapping (always run as robust fallback)
    ocr_matches = ocr_find_text_bbox(screenshot_b64, q)
    fallback_info["ocr_matches"] = ocr_matches
    mapping = map_ocr_to_candidates(ocr_matches, canonical_nodes)
    # choose candidate with highest IoU across OCR matches
    best_iou = 0.0
    best_idx_by_ocr = None
    for (_, cand_idx, iou) in mapping:
        if cand_idx is not None and iou > best_iou:
            best_iou = iou
            best_idx_by_ocr = cand_idx

    if best_idx_by_ocr is not None:
        return {
            "best_index": int(best_idx_by_ocr),
            "best_canonical": flatten_canonical_item(canonical_nodes[best_idx_by_ocr]),
            "best_node": canonical_nodes[best_idx_by_ocr],
            "scores": sims.tolist(),
            "fallback_used": True,
            "fallback": fallback_info,
            "diagnostics": diagnostics
        }

    # If OCR mapping didn't help, return fallback info with top candidate anyway
    return {
        "best_index": best_idx,
        "best_canonical": canonical_strings[best_idx],
        "best_node": canonical_nodes[best_idx],
        "scores": sims.tolist(),
        "fallback_used": True,
        "fallback": fallback_info,
        "diagnostics": diagnostics
    }

# ---------------------------
# 8) Example usage (run)
# ---------------------------
if __name__ == "__main__":
    # Example: change URL and target_text as needed
    url = "https://www.verizon.com/smartphones/"
    target_text = "Compare"   # can use full phrase or short token
    result = retrieve_best_element(url, target_text,
                                   query="Apple iPhone 17 Pro Max device",
                                   ancestor_levels=2,
                                   max_siblings=1,
                                   include_hidden=False,
                                   similarity_threshold=SIMILARITY_THRESHOLD,
                                   ambiguity_delta=AMBIGUITY_DELTA)
    # Print concise output
    print(json.dumps({
        "best_index": result["best_index"],
        "best_canonical": (result["best_canonical"][:1000] + "...") if result["best_canonical"] and len(result["best_canonical"])>1000 else result["best_canonical"],
        "scores_top": result["scores"][:5] if result["scores"] else result["scores"],
        "fallback_used": result["fallback_used"],
        "fallback_info_keys": list(result["fallback"].keys()) if result["fallback"] else None,
        "diagnostics": result["diagnostics"]
    }, indent=2))